{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03656b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707c2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "import pathlib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3becd",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914bcc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload_date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>13.36</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.26</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>6.80</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.41</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.08</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>6.03</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>5.56</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>5.48</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>5.03</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>4.97</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>officialpsy</td>\n",
       "      <td>4.90</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.56</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>4.44</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>4.06</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.93</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[39]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.87</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Roar\"[40]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.87</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>3.78</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.73</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Sorry\"[43]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.71</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[44]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.69</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Thinking Out Loud\"[45]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.66</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.59</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.55</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[48]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.54</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Faded\"[49]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.51</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Let Her Go\"[50]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.51</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.48</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[52]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>3.46</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[53]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.45</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                             \"Counting Stars\"[39]   \n",
       "16  17.                                       \"Roar\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "19  20.                                      \"Sorry\"[43]   \n",
       "20  21.                             \"Lakdi Ki Kathi\"[44]   \n",
       "21  22.                          \"Thinking Out Loud\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.          \"Humpty the train on a fruits ride\"[47]   \n",
       "24  25.                                    \"Perfect\"[48]   \n",
       "25  26.                                      \"Faded\"[49]   \n",
       "26  27.                                 \"Let Her Go\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                    \"Lean On\"[52]   \n",
       "29  30.                                   \"Bailando\"[53]   \n",
       "\n",
       "                                               Artist Upload_date  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories       13.36   \n",
       "1                                          Luis Fonsi        8.26   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs        6.80   \n",
       "3                          Cocomelon - Nursery Rhymes        6.41   \n",
       "4                                          Ed Sheeran        6.08   \n",
       "5                                         Wiz Khalifa        6.03   \n",
       "6                          Cocomelon - Nursery Rhymes        5.56   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs        5.48   \n",
       "8                                         Mark Ronson        5.03   \n",
       "9                                         Miroshka TV        4.97   \n",
       "10                                        officialpsy        4.90   \n",
       "11                                         Get Movies        4.56   \n",
       "12                                      Ultra Records        4.44   \n",
       "13                                         Crazy Frog        4.06   \n",
       "14                                           Maroon 5        3.93   \n",
       "15                                        OneRepublic        3.87   \n",
       "16                                         Katy Perry        3.87   \n",
       "17                         Cocomelon - Nursery Rhymes        3.78   \n",
       "18                                            Shakira        3.73   \n",
       "19                                      Justin Bieber        3.71   \n",
       "20                                       Jingle Toons        3.69   \n",
       "21                                         Ed Sheeran        3.66   \n",
       "22                                         Katy Perry        3.59   \n",
       "23      Kiddiestv Hindi - Nursery Rhymes & Kids Songs        3.55   \n",
       "24                                         Ed Sheeran        3.54   \n",
       "25                                        Alan Walker        3.51   \n",
       "26                                          Passenger        3.51   \n",
       "27                                           Maroon 5        3.48   \n",
       "28                               Major Lazer Official        3.46   \n",
       "29                                   Enrique Iglesias        3.45   \n",
       "\n",
       "                Views  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4    January 30, 2017  \n",
       "5       April 6, 2015  \n",
       "6        May 24, 2018  \n",
       "7       March 6, 2014  \n",
       "8   November 19, 2014  \n",
       "9   February 27, 2018  \n",
       "10      July 15, 2012  \n",
       "11   January 31, 2012  \n",
       "12      April 5, 2018  \n",
       "13      June 16, 2009  \n",
       "14   January 14, 2015  \n",
       "15       May 31, 2013  \n",
       "16  September 5, 2013  \n",
       "17      June 25, 2018  \n",
       "18       June 4, 2010  \n",
       "19   October 22, 2015  \n",
       "20      June 14, 2018  \n",
       "21    October 7, 2014  \n",
       "22  February 20, 2014  \n",
       "23   January 26, 2018  \n",
       "24   November 9, 2017  \n",
       "25   December 3, 2015  \n",
       "26      July 25, 2012  \n",
       "27       May 31, 2018  \n",
       "28     March 22, 2015  \n",
       "29     April 11, 2014  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining driver and url\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "time.sleep(5)\n",
    "# defining empty list for finding variable\n",
    "rank =[]\n",
    "name = []\n",
    "artist = []\n",
    "upload_date = []\n",
    "views =[]\n",
    "# Using try and Except method\n",
    "try:\n",
    "    rank_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "    for i in rank_tags:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "    \n",
    "try:\n",
    "    name_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')\n",
    "    for i in name_tags:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "    \n",
    "try:\n",
    "    artist_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]')\n",
    "    for i in artist_tags:\n",
    "        artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    artist.append('-')\n",
    "    \n",
    "try:\n",
    "    date_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]')\n",
    "    for i in date_tags:\n",
    "        upload_date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    upload_date.append('-')\n",
    "\n",
    "try:\n",
    "    views_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]')\n",
    "    for i in views_tags:\n",
    "        views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    views.append('-')\n",
    "    \n",
    "\n",
    "print(len(rank),len(name),len(artist),len(upload_date), len(views))\n",
    "\n",
    "# Converting it in dataframe\n",
    "Most_Viewed_youtube = pd.DataFrame({'Rank':rank, 'Name':name, 'Artist':artist, 'Upload_date':upload_date,'Views':views})\n",
    "Most_Viewed_youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189740d",
   "metadata": {},
   "source": [
    "2.Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/. You need to find following details: A) Match title (I.e. 1 ODI) B) Series C) Place D) Date E) Time Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5422409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match_Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Match_Title, Series, Place, Date, Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining driver and url\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "time.sleep(5)\n",
    "\n",
    "tab = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]')\n",
    "tab.click()\n",
    "# defining empty list for finding variable\n",
    "title =[]\n",
    "series =[]\n",
    "place =[]\n",
    "date = []\n",
    "times = []\n",
    "\n",
    "# Using Try and except method\n",
    "try:\n",
    "    title_tags = driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "    for i in title_tags:\n",
    "        title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    title.append('-')\n",
    "try:\n",
    "    series_tags = driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "    for i in series_tags:\n",
    "        series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    series.append('-')\n",
    "try:\n",
    "    place_tags = driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]')\n",
    "    for i in place_tags:\n",
    "        place.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    place.append('-')\n",
    "try:\n",
    "    date_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "    for i in date_tags:\n",
    "        date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    date.append('-')\n",
    "try:\n",
    "    time_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "    for i in time_tags:\n",
    "        times.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    times.append('-')\n",
    "\n",
    "# Converting it in dataframe\n",
    "print(len(title),len(series),len(place),len(date), len(times))\n",
    "ranking = pd.DataFrame({'Match_Title':title, 'Series':series, 'Place':place, 'Date':date,'Time':times})\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3885c07",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A)\n",
    "Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b17a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\"}\n  (Session info: chrome=117.0.5938.89); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6F8FA78A2+54818]\n\t(No symbol) [0x00007FF6F8F16AD2]\n\t(No symbol) [0x00007FF6F8DCDA3B]\n\t(No symbol) [0x00007FF6F8E0E4FC]\n\t(No symbol) [0x00007FF6F8E0E67C]\n\t(No symbol) [0x00007FF6F8E49627]\n\t(No symbol) [0x00007FF6F8E2EAEF]\n\t(No symbol) [0x00007FF6F8E475A2]\n\t(No symbol) [0x00007FF6F8E2E883]\n\t(No symbol) [0x00007FF6F8E03691]\n\t(No symbol) [0x00007FF6F8E048D4]\n\tGetHandleVerifier [0x00007FF6F930B9A2+3610402]\n\tGetHandleVerifier [0x00007FF6F9361870+3962352]\n\tGetHandleVerifier [0x00007FF6F9359D5F+3930847]\n\tGetHandleVerifier [0x00007FF6F9043656+693206]\n\t(No symbol) [0x00007FF6F8F21638]\n\t(No symbol) [0x00007FF6F8F1D944]\n\t(No symbol) [0x00007FF6F8F1DA72]\n\t(No symbol) [0x00007FF6F8F0E123]\n\tBaseThreadInitThunk [0x00007FFF328D8364+20]\n\tRtlUserThreadStart [0x00007FFF352DE851+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m economy\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m      8\u001b[0m india\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m----> 9\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m states\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# defining empty list for finding variable\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:738\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    735\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    736\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\"}\n  (Session info: chrome=117.0.5938.89); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6F8FA78A2+54818]\n\t(No symbol) [0x00007FF6F8F16AD2]\n\t(No symbol) [0x00007FF6F8DCDA3B]\n\t(No symbol) [0x00007FF6F8E0E4FC]\n\t(No symbol) [0x00007FF6F8E0E67C]\n\t(No symbol) [0x00007FF6F8E49627]\n\t(No symbol) [0x00007FF6F8E2EAEF]\n\t(No symbol) [0x00007FF6F8E475A2]\n\t(No symbol) [0x00007FF6F8E2E883]\n\t(No symbol) [0x00007FF6F8E03691]\n\t(No symbol) [0x00007FF6F8E048D4]\n\tGetHandleVerifier [0x00007FF6F930B9A2+3610402]\n\tGetHandleVerifier [0x00007FF6F9361870+3962352]\n\tGetHandleVerifier [0x00007FF6F9359D5F+3930847]\n\tGetHandleVerifier [0x00007FF6F9043656+693206]\n\t(No symbol) [0x00007FF6F8F21638]\n\t(No symbol) [0x00007FF6F8F1D944]\n\t(No symbol) [0x00007FF6F8F1DA72]\n\t(No symbol) [0x00007FF6F8F0E123]\n\tBaseThreadInitThunk [0x00007FFF328D8364+20]\n\tRtlUserThreadStart [0x00007FFF352DE851+33]\n"
     ]
    }
   ],
   "source": [
    "# defining driver and url\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "economy = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button')\n",
    "india = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')\n",
    "economy.click()\n",
    "india.click()\n",
    "states = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "states.click()\n",
    "# defining empty list for finding variable\n",
    "rank =[]\n",
    "state = []\n",
    "gsdp_1920 = []\n",
    "gsdp_1819 = []\n",
    "share = []\n",
    "gdp = []\n",
    "# Using Try and except method\n",
    "try:\n",
    "    rank_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "    for i in rank_tags:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "\n",
    "try:\n",
    "    state_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "    for i in state_tags:\n",
    "        state.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    state.append('-')\n",
    "\n",
    "try:\n",
    "    gsdp19_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "    for i in gsdp19_tags:\n",
    "        gsdp_1920.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp_1920.append('-')\n",
    "\n",
    "try:\n",
    "    gsdp18_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "    for i in gsdp18_tags:\n",
    "        gsdp_1819.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp_1819.append('-')\n",
    "try:\n",
    "    share_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "    for i in share_tags:\n",
    "        share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    share.append('-')\n",
    "try:\n",
    "    gdp_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "    for i in gdp_tags:\n",
    "        gdp.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gdp.append('-')\n",
    "\n",
    "print(len(rank),len(state),len(gsdp_1920),len(gsdp_1819), len(share),len(gdp))\n",
    "\n",
    "# Converting it in dataframe\n",
    "GDP_details = pd.DataFrame({'Rank':rank, 'States':state, 'GSDP 19-20':gsdp_1920, 'GSDP 18-19':gsdp_1819,'Share':share,'GDP':gdp})\n",
    "GDP_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cacfc",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used \n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3b05d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 24 25 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyperdxio / hyperdx</td>\n",
       "      <td>Resolve production issues, fast. An open sourc...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[TypeScript, SCSS, Dockerfile, Python, JavaScr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>williamyang1991 / Rerender_A_Video</td>\n",
       "      <td>[SIGGRAPH Asia 2023] Rerender A Video: Zero-Sh...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[Jupyter Notebook, Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NExT-GPT / NExT-GPT</td>\n",
       "      <td>Code and models for NExT-GPT: Any-to-Any Multi...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makeplane / plane</td>\n",
       "      <td>🔥 🔥 🔥 Open Source JIRA, Linear and Height Alte...</td>\n",
       "      <td>[48]</td>\n",
       "      <td>[TypeScript, Python, HTML, CSS, JavaScript, Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grafana / beyla</td>\n",
       "      <td>eBPF-based autoinstrumentation of HTTP and HTT...</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[C, Go, Ruby, Dockerfile, Makefile, Rust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CorentinJ / Real-Time-Voice-Cloning</td>\n",
       "      <td>Clone a voice in 5 seconds to generate arbitra...</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mastodon / mastodon</td>\n",
       "      <td>Your self-hosted, globally interconnected micr...</td>\n",
       "      <td>[867]</td>\n",
       "      <td>[Ruby, JavaScript, SCSS, Haml, TypeScript, HTML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AntonioErdeljac / next13-lms-platform</td>\n",
       "      <td>Self-hosted AI coding assistant</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[TypeScript, JavaScript, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TabbyML / tabby</td>\n",
       "      <td>Deploy web apps anywhere.</td>\n",
       "      <td>[18]</td>\n",
       "      <td>[TypeScript, Rust, Kotlin, Python, Vim Script,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>basecamp / kamal</td>\n",
       "      <td>Godot Engine – Multi-platform 2D and 3D game e...</td>\n",
       "      <td>[61]</td>\n",
       "      <td>[Ruby, Dockerfile, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>godotengine / godot</td>\n",
       "      <td>The React Framework</td>\n",
       "      <td>[2,209]</td>\n",
       "      <td>[C++, C#, C, GLSL, Java, Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vercel / next.js</td>\n",
       "      <td>A list of SaaS, PaaS and IaaS offerings that h...</td>\n",
       "      <td>[2,887]</td>\n",
       "      <td>[JavaScript, TypeScript, Rust, MDX, CSS, SCSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ripienaar / free-for-dev</td>\n",
       "      <td>An Open-source Framework for Autonomous Langua...</td>\n",
       "      <td>[1,483]</td>\n",
       "      <td>[HTML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>aiwaves-cn / agents</td>\n",
       "      <td>Data set of top third party web domains with r...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>duckduckgo / tracker-radar</td>\n",
       "      <td>Elegant HTTP Networking in Swift</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[JavaScript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alamofire / Alamofire</td>\n",
       "      <td>🐸💬 - a deep learning toolkit for Text-to-Speec...</td>\n",
       "      <td>[256]</td>\n",
       "      <td>[Swift]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>coqui-ai / TTS</td>\n",
       "      <td>[ICCV 2023] ProPainter: Improving Propagation ...</td>\n",
       "      <td>[130]</td>\n",
       "      <td>[Python, Jupyter Notebook, HTML, Shell, Makefi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sczhou / ProPainter</td>\n",
       "      <td>Toy Gaussian Splatting visualization in Unity</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aras-p / UnityGaussianSplatting</td>\n",
       "      <td>「Java学习+面试指南」一份涵盖大部分 Java 程序员所需要掌握的核心知识。准备 Jav...</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[C#, C++, C, HLSL, ShaderLab, GLSL, CMake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Snailclimb / JavaGuide</td>\n",
       "      <td>The paper list of the 86-page paper \"The Rise ...</td>\n",
       "      <td>[452]</td>\n",
       "      <td>[Java, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WooooDyy / LLM-Agent-Paper-List</td>\n",
       "      <td>A list of awesome beginners-friendly projects.</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MunGell / awesome-for-beginners</td>\n",
       "      <td>A modern formatting library</td>\n",
       "      <td>[279]</td>\n",
       "      <td>[-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fmtlib / fmt</td>\n",
       "      <td>Meshery, the cloud native manager</td>\n",
       "      <td>[421]</td>\n",
       "      <td>[C++, CMake, Python, Shell, Cuda, Starlark]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>meshery / meshery</td>\n",
       "      <td>⚡ Building applications with LLMs through comp...</td>\n",
       "      <td>[498]</td>\n",
       "      <td>[Go, JavaScript, CSS, Mustache, Makefile, Open...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Repository Title  \\\n",
       "0                     hyperdxio / hyperdx   \n",
       "1      williamyang1991 / Rerender_A_Video   \n",
       "2                     NExT-GPT / NExT-GPT   \n",
       "3                       makeplane / plane   \n",
       "4                         grafana / beyla   \n",
       "5     CorentinJ / Real-Time-Voice-Cloning   \n",
       "6                     mastodon / mastodon   \n",
       "7   AntonioErdeljac / next13-lms-platform   \n",
       "8                         TabbyML / tabby   \n",
       "9                        basecamp / kamal   \n",
       "10                    godotengine / godot   \n",
       "11                       vercel / next.js   \n",
       "12               ripienaar / free-for-dev   \n",
       "13                    aiwaves-cn / agents   \n",
       "14             duckduckgo / tracker-radar   \n",
       "15                  Alamofire / Alamofire   \n",
       "16                         coqui-ai / TTS   \n",
       "17                    sczhou / ProPainter   \n",
       "18        aras-p / UnityGaussianSplatting   \n",
       "19                 Snailclimb / JavaGuide   \n",
       "20        WooooDyy / LLM-Agent-Paper-List   \n",
       "21        MunGell / awesome-for-beginners   \n",
       "22                           fmtlib / fmt   \n",
       "23                      meshery / meshery   \n",
       "\n",
       "                               Repository Description Contributors count  \\\n",
       "0   Resolve production issues, fast. An open sourc...                [2]   \n",
       "1   [SIGGRAPH Asia 2023] Rerender A Video: Zero-Sh...                [2]   \n",
       "2   Code and models for NExT-GPT: Any-to-Any Multi...                [4]   \n",
       "3   🔥 🔥 🔥 Open Source JIRA, Linear and Height Alte...               [48]   \n",
       "4   eBPF-based autoinstrumentation of HTTP and HTT...               [17]   \n",
       "5   Clone a voice in 5 seconds to generate arbitra...               [17]   \n",
       "6   Your self-hosted, globally interconnected micr...              [867]   \n",
       "7                     Self-hosted AI coding assistant                [-]   \n",
       "8                           Deploy web apps anywhere.               [18]   \n",
       "9   Godot Engine – Multi-platform 2D and 3D game e...               [61]   \n",
       "10                                The React Framework            [2,209]   \n",
       "11  A list of SaaS, PaaS and IaaS offerings that h...            [2,887]   \n",
       "12  An Open-source Framework for Autonomous Langua...            [1,483]   \n",
       "13  Data set of top third party web domains with r...               [14]   \n",
       "14                   Elegant HTTP Networking in Swift               [17]   \n",
       "15  🐸💬 - a deep learning toolkit for Text-to-Speec...              [256]   \n",
       "16  [ICCV 2023] ProPainter: Improving Propagation ...              [130]   \n",
       "17      Toy Gaussian Splatting visualization in Unity                [2]   \n",
       "18  「Java学习+面试指南」一份涵盖大部分 Java 程序员所需要掌握的核心知识。准备 Jav...                [-]   \n",
       "19  The paper list of the 86-page paper \"The Rise ...              [452]   \n",
       "20     A list of awesome beginners-friendly projects.               [14]   \n",
       "21                        A modern formatting library              [279]   \n",
       "22                  Meshery, the cloud native manager              [421]   \n",
       "23  ⚡ Building applications with LLMs through comp...              [498]   \n",
       "\n",
       "                                        Language used  \n",
       "0   [TypeScript, SCSS, Dockerfile, Python, JavaScr...  \n",
       "1                          [Jupyter Notebook, Python]  \n",
       "2                                     [Python, Shell]  \n",
       "3   [TypeScript, Python, HTML, CSS, JavaScript, Do...  \n",
       "4           [C, Go, Ruby, Dockerfile, Makefile, Rust]  \n",
       "5                                            [Python]  \n",
       "6    [Ruby, JavaScript, SCSS, Haml, TypeScript, HTML]  \n",
       "7                       [TypeScript, JavaScript, CSS]  \n",
       "8   [TypeScript, Rust, Kotlin, Python, Vim Script,...  \n",
       "9                           [Ruby, Dockerfile, Shell]  \n",
       "10                   [C++, C#, C, GLSL, Java, Python]  \n",
       "11     [JavaScript, TypeScript, Rust, MDX, CSS, SCSS]  \n",
       "12                                             [HTML]  \n",
       "13                                           [Python]  \n",
       "14                                       [JavaScript]  \n",
       "15                                            [Swift]  \n",
       "16  [Python, Jupyter Notebook, HTML, Shell, Makefi...  \n",
       "17                                           [Python]  \n",
       "18         [C#, C++, C, HLSL, ShaderLab, GLSL, CMake]  \n",
       "19                                      [Java, Shell]  \n",
       "20                                                [-]  \n",
       "21                                                [-]  \n",
       "22        [C++, CMake, Python, Shell, Cuda, Starlark]  \n",
       "23  [Go, JavaScript, CSS, Mustache, Makefile, Open...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining driver and url\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://github.com/\")\n",
    "time.sleep(5)\n",
    "\n",
    "src = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "src.click()\n",
    "trending = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "trending.click()\n",
    "time.sleep(10)\n",
    "# defining empty list for finding variable\n",
    "title = []\n",
    "desc = []\n",
    "count = []\n",
    "lang = []\n",
    "\n",
    "URL= []\n",
    "link=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]/a')\n",
    "for i in link:\n",
    "    URL.append(i.get_attribute('href'))\n",
    "# Using Try and except method\n",
    "try:\n",
    "    title_tags = driver.find_elements(By.XPATH,'/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/h2')\n",
    "    for i in title_tags:\n",
    "        title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    title.append('-')\n",
    "\n",
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        desc_tags = driver.find_elements(By.XPATH,'//*[@class=\"f4 my-3\" or @class= \"f4 my-3 color-fg-muted text-italic\"]')\n",
    "        for i in desc_tags:\n",
    "            desc.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        desc.append('-')\n",
    "    countlist = []\n",
    "    try:\n",
    "        count_tags = driver.find_elements(By.XPATH,'//h2[@class=\"h4 mb-3\"]/a[contains(text(),\"Contributors\")]/span')\n",
    "        if count_tags:\n",
    "            for j in count_tags:\n",
    "                countlist.append(j.text)\n",
    "        else:\n",
    "            countlist.append('-')\n",
    "        count.append(countlist)\n",
    "    except NoSuchElementException:\n",
    "        count.append('-')\n",
    "    langlist =[]\n",
    "    try:\n",
    "        lang_tags = driver.find_elements(By.XPATH,'//li[@class=\"d-inline\"]//a//span[1]')\n",
    "        if lang_tags:\n",
    "            for j in lang_tags:\n",
    "                langlist.append(j.text)\n",
    "        else:\n",
    "            langlist.append('-')\n",
    "        lang.append(langlist)\n",
    "    except NoSuchElementException:\n",
    "        lang.append('-')\n",
    "\n",
    "print(len(title),len(desc),len(count),len(lang))\n",
    "# Converting it in dataframe\n",
    "GITHUB = pd.DataFrame({'Repository Title':title[0:24], \n",
    "                    'Repository Description':desc[0:24],\n",
    "                    'Contributors count':count[0:24],\n",
    "                    'Language used':lang[0:24]})\n",
    "GITHUB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ebabd",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89302d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://www.billboard.com/\")\n",
    "\n",
    "charts = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "charts.click()\n",
    "\n",
    "hot100 = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/main/div[2]/div[1]/div[1]/div/div/div[3]/a')\n",
    "hot100.click()\n",
    "time.sleep(5)\n",
    "\n",
    "song_name = []\n",
    "artist_name = []\n",
    "lastweek_rank =[]\n",
    "peak_rank = []\n",
    "weekson_board = []\n",
    "\n",
    "try:\n",
    "    sname = driver.find_elements(By.XPATH, '//h3[@class = \"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 u-font-size-23@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-245 u-max-width-230@tablet-only u-letter-spacing-0028@tablet\" or @class = \"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 lrv-u-font-size-18@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "    for i in sname:\n",
    "        song_name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    song_name.append('-')\n",
    "\n",
    "try:\n",
    "    aname = driver.find_elements(By.XPATH,'//span[@class = \"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet\" or @class = \"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "    for i in aname:\n",
    "        artist_name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    artist_name.append('-')\n",
    "    \n",
    "\n",
    "try:\n",
    "    lrank = driver.find_elements(By.XPATH, '//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[4]')\n",
    "    for i in lrank:\n",
    "        lastweek_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lastweek_rank.append('-')\n",
    "\n",
    "\n",
    "try:\n",
    "    prank = driver.find_elements(By.XPATH,'//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[5]')\n",
    "    for i in prank:\n",
    "        peak_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    peak_rank.append('-')\n",
    "\n",
    "try:\n",
    "    week = driver.find_elements(By.XPATH,'//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[6]')\n",
    "    for i in week:\n",
    "        weekson_board.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    weekson_board.append('-')\n",
    "    \n",
    "print(len(song_name),len(artist_name),len(lastweek_rank),len(peak_rank),len(weekson_board))\n",
    "top100 = pd.DataFrame({'Song Name':song_name,'Artist Name': artist_name,'Last Week Rank':lastweek_rank,'Peak Rank':peak_rank,'Weeks on Chart':weekson_board})\n",
    "top100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455235b",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "compare\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "\n",
    "url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "bname = []\n",
    "aname = []\n",
    "volume = []\n",
    "publisher = []\n",
    "genre = []\n",
    "\n",
    "try:\n",
    "    book = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[2]')\n",
    "    for i in book:\n",
    "        bname.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    bname.append('-')\n",
    "try:\n",
    "    auth = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[3]')\n",
    "    for i in auth:\n",
    "        aname.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    aname.append('-')\n",
    "try:\n",
    "    vol = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[4]')\n",
    "    for i in vol:\n",
    "        volume.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    volume.append('-')\n",
    "try:\n",
    "    pub = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[5]')\n",
    "    for i in pub:\n",
    "        publisher.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    publisher.append('-')\n",
    "try:\n",
    "    gen = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[6]')\n",
    "    for i in gen:\n",
    "        genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    genre.append('-')\n",
    "\n",
    "print(len(bname),len(aname),len(volume),len(publisher),len(genre))\n",
    "print('\\033[1m'+'Highest selling novels:'+'\\033[0m')\n",
    "bestselling = pd.DataFrame({'Book Name':bname,'Artist Name': aname,'Volumes Sold':volume,'Publisher':publisher,'Genre':genre})\n",
    "bestselling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466030be",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You\n",
    "have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "name = []\n",
    "year = []\n",
    "genres = []\n",
    "runtime = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "try:\n",
    "    name_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/h3/a')\n",
    "    for i in name_tags:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "try:\n",
    "    year_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/h3/span[2]')\n",
    "    for i in year_tags:\n",
    "        year.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    year.append('-')\n",
    "try:\n",
    "    genre_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[1]/span[5]')\n",
    "    for i in genre_tags:\n",
    "        genres.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    genres.append('-')\n",
    "try:\n",
    "    run_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[1]/span[3]')\n",
    "    for i in run_tags:\n",
    "        runtime.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    runtime.append('-')\n",
    "try:\n",
    "    rating_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/div[1]/div[1]/span[2]')\n",
    "    for i in rating_tags:\n",
    "        ratings.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    ratings.append('-')\n",
    "try:\n",
    "    vote_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[4]/span[2]')\n",
    "    for i in vote_tags:\n",
    "        votes.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    votes.append('-')\n",
    "\n",
    "print(len(name),len(year),len(genres),len(runtime),len(ratings),len(votes))\n",
    "\n",
    "\n",
    "IMDB_Data = pd.DataFrame({'Show Name':name,'Year': year,'Genre':genres,'Runtime':runtime, 'Rating':ratings,'Votes':votes})\n",
    "IMDB_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb7488",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15035e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "time.sleep(5)\n",
    "\n",
    "view = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "view.click()\n",
    "\n",
    "time.sleep(20)\n",
    "accept = driver.find_element(By.XPATH,'//*[@class = \"btn-primary btn-sm btn m-1\"]')\n",
    "accept.click()\n",
    "time.sleep (30)\n",
    "\n",
    "expand_all = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]/div[2]/span[2]')\n",
    "expand_all.click()\n",
    "time.sleep(20)\n",
    "\n",
    "name =[]\n",
    "data_type =[]\n",
    "task_type = []\n",
    "#attribute_type = []\n",
    "#no_instances = []\n",
    "no_attribute = []\n",
    "#year = []\n",
    "\n",
    "for page in range(0,63):\n",
    "    driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "    try:\n",
    "        name_tags = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/h2/a')\n",
    "        for i in name_tags:\n",
    "            name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append('-')\n",
    "\n",
    "    try:\n",
    "        data = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[2]/span')\n",
    "        for i in data:\n",
    "            data_type.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        data_type.append('-')\n",
    "\n",
    "    try:\n",
    "        task = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[1]/span')\n",
    "        for i in task:\n",
    "            task_type.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        task_type.append('-')\n",
    "    try:\n",
    "        attribute_tags = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[4]/span')\n",
    "        for i in attribute_tags:\n",
    "            no_attribute.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        no_attribute.append('-')\n",
    "    if(page == 10):\n",
    "        break\n",
    "    else:\n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//div[@class =\"btn-group\"]/button[2]'))).click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "print(len(name),len(data_type),len(task_type),len(no_attribute))\n",
    "\n",
    "uci = pd.DataFrame({'Dataset Name':name,'Data Type':data_type, 'Task': task_type,'Number of attributes':no_attribute})\n",
    "uci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79248681",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
